{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50c376ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "#from utils import CustomDataset, get_model, validate, data_loader\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from timm.models.efficientnet import *\n",
    "import torchvision.models as models\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import albumentations as A\n",
    "\n",
    "import time\n",
    "\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "from monai.networks.nets import UNet, UNETR\n",
    "import dataset\n",
    "\n",
    "from models.ternausnets import AlbuNet\n",
    "from models.selim_zoo.unet import Resnet\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from monai.networks.nets import UNet\n",
    "from models import ternausnets\n",
    "from models.selim_zoo.unet import Resnet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f07dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):#info about img + info about mask\n",
    "\n",
    "    def __init__(self, model, vec_length=256, img_size=1024, weight_mask=0.8):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.weight_mask = weight_mask\n",
    "        self.in_features = 675 if img_size == 1024 else 147\n",
    "        self.conv_block = nn.Sequential(\n",
    "            nn.Conv2d(1, 8, kernel_size=3,stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, stride=2),\n",
    "            \n",
    "            nn.Conv2d(8, 16, kernel_size=5, stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 6, kernel_size=3,stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Conv2d(6, 3, kernel_size=2,stride=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        self.linear_block = nn.Sequential(\n",
    "            nn.Linear(self.in_features, 300),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            nn.Linear(300, vec_length),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "        )\n",
    "    def forward_once(self, x):\n",
    "        pred = self.model(x)\n",
    "        res = self.conv_block(pred)\n",
    "        res_img = self.conv_block(x[:,0:1,:,:])\n",
    "        mask_vec = self.linear_block(res)\n",
    "        img_vec = self.linear_block(res_img)\n",
    "        return pred, mask_vec+img_vec#self.weight_mask * mask_vec + (1 - self.weight_mask) * img_vec\n",
    "\n",
    "    def forward(self, x_mlo, x_cc):\n",
    "        x_mlo, x_cc = x_mlo.float(), x_cc.float()\n",
    "\n",
    "        pred_mlo, vec_mlo = self.forward_once(x_mlo)\n",
    "        pred_cc, vec_cc = self.forward_once(x_cc)\n",
    "        return pred_mlo, pred_cc, vec_mlo, vec_cc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daba8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(\n",
    "    spatial_dims=2,\n",
    "    in_channels=3,\n",
    "    out_channels=1,\n",
    "    channels=(4, 8, 16, 32, 64),\n",
    "    strides=(2, 2, 2, 2),\n",
    ")\n",
    "print(f'before {sum(p.numel() for p in model.parameters())}')\n",
    "model = MyModel(model, img_size=512)\n",
    "\n",
    "print(f'after {sum(p.numel() for p in model.parameters())}')\n",
    "\n",
    "model.load_state_dict(torch.load('./best_models/unet_fold0_metric1.107_weight0.9_stop157.pth')['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "2da8f53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.imshow(np.moveaxis(img_mlo[0].detach().cpu().numpy(), 0, 2))\n",
    "#plt.imshow(img_mlo[0][2].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f46253e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(torch.device('cuda:1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "0a49d306",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLoss(torch.nn.Module):\n",
    "    def __init__(self, loss_weight=0.5):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.loss_weight=loss_weight\n",
    "\n",
    "    def forward(self, output_mlo, output_cc):\n",
    "      euclidean_distance = F.pairwise_distance(output_mlo, output_cc)\n",
    "      loss = torch.mean(torch.pow(euclidean_distance, 2))\n",
    "      return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb19c8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def binarize_mask(mask, threshold=0.5):\n",
    "    mask_sigm = torch.sigmoid(mask)\n",
    "    mask_binar = torch.where(mask_sigm > threshold, torch.ones_like(mask_sigm), torch.zeros_like(mask_sigm))\n",
    "    return mask_binar\n",
    "\n",
    "def plot_res(res_mlo, res_cc, label_mlo, label_cc):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(np.moveaxis(binarize_mask(res_mlo)[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('predicted mlo')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(np.moveaxis(binarize_mask(res_cc)[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('predicted cc')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(np.moveaxis(label_mlo[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('true mlo')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(np.moveaxis(label_cc[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('true cc')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_img(img_mlo, img_cc, label_mlo, label_cc):\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(np.moveaxis(img_mlo[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('img mlo')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.imshow(np.moveaxis(img_cc[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('img cc')\n",
    "    plt.axis('off')\n",
    "\n",
    "\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.imshow(np.moveaxis(label_mlo[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('mask mlo')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.imshow(np.moveaxis(label_cc[0].detach().cpu().numpy(), 0, 2))\n",
    "    plt.title('mask cc')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "train_transform = './transforms/train_transform512.yml' \n",
    "val_transform = './transforms/val_transform512.yml'\n",
    "\n",
    "train_loader, valid_loader, test_loader = dataset.data_loader(batch_size=4, train_transform=train_transform, val_transform=val_transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0de7adf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_mlo, img_cc, mask1, mask2 in train_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f326505d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ae339",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_mlo, img_cc, mask1, mask2 in train_loader:\n",
    "    img_mlo, img_cc = img_mlo.to(torch.device('cuda:1')), img_cc.to(torch.device('cuda:1'))\n",
    "    img_mlo, img_cc = img_mlo.float(), img_cc.float()\n",
    "    pred_mlo, pred_cc, vec_mlo, vec_cc = model(img_mlo, img_cc)\n",
    "    plot_res(img_mlo, img_cc, pred_mlo, pred_cc)\n",
    "    clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8686b95f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch_dct\n",
    "import scipy\n",
    "\n",
    "class DctMaskEncoding(object):\n",
    "    \"\"\"\n",
    "    Apply DCT to encode the binary mask, and use the encoded vector as mask representation in instance segmentation.\n",
    "    \"\"\"\n",
    "    def __init__(self, vec_dim, mask_size=128):\n",
    "        \"\"\"\n",
    "        vec_dim: the dimension of the encoded vector, int\n",
    "        mask_size: the resolution of the initial binary mask representaiton.\n",
    "        \"\"\"\n",
    "        self.vec_dim = vec_dim\n",
    "        self.mask_size = mask_size\n",
    "        assert vec_dim <= mask_size*mask_size\n",
    "        self.dct_vector_coords = self.get_dct_vector_coords(r=mask_size)\n",
    "        #print(self.dct_vector_coords.shape)\n",
    "\n",
    "    def encode(self, masks, dim=None):\n",
    "        \"\"\"\n",
    "        Encode the mask to vector of vec_dim or specific dimention.\n",
    "        \"\"\"\n",
    "        if dim is None:\n",
    "            dct_vector_coords = self.dct_vector_coords[:self.vec_dim]\n",
    "        else:\n",
    "            dct_vector_coords = self.dct_vector_coords[:dim]\n",
    "        masks = masks.view([-1, self.mask_size, self.mask_size]).to(dtype=float)  # [N, H, W]\n",
    "        #print(masks.shape)\n",
    "        dct_all = scipy.fftpack.dctn(masks.numpy(), norm='ortho')\n",
    "        dct_all = torch.from_numpy(dct_all)\n",
    "        xs, ys = dct_vector_coords[:, 0], dct_vector_coords[:, 1]\n",
    "        #print(dct_all.shape)\n",
    "        dct_vectors = dct_all[:, xs, ys]  # reshape as vector\n",
    "        return dct_vectors  # [N, D]\n",
    "\n",
    "    def decode(self, dct_vectors, dim=None):\n",
    "        \"\"\"\n",
    "        intput: dct_vector numpy [N,dct_dim]\n",
    "        output: mask_rc mask reconstructed [N, mask_size, mask_size]\n",
    "        \"\"\"\n",
    "        device = dct_vectors.device\n",
    "        if dim is None:\n",
    "            dct_vector_coords = self.dct_vector_coords[:self.vec_dim]\n",
    "        else:\n",
    "            dct_vector_coords = self.dct_vector_coords[:dim]\n",
    "            dct_vectors = dct_vectors[:, :dim]\n",
    "\n",
    "        N = dct_vectors.shape[0]\n",
    "        dct_trans = torch.zeros([N, self.mask_size, self.mask_size], dtype=dct_vectors.dtype).to(device)\n",
    "        xs, ys = dct_vector_coords[:, 0], dct_vector_coords[:, 1]\n",
    "        dct_trans[:, xs, ys] = dct_vectors\n",
    "        mask_rc = scipy.fftpack.idctn(dct_trans.numpy(), norm='ortho')  # [N, mask_size, mask_size]\n",
    "        mask_rc = torch.from_numpy(mask_rc)\n",
    "        return mask_rc\n",
    "\n",
    "    def get_dct_vector_coords(self, r=128):\n",
    "        \"\"\"\n",
    "        Get the coordinates with zigzag order.\n",
    "        \"\"\"\n",
    "        dct_index = []\n",
    "        for i in range(r):\n",
    "            if i % 2 == 0:  # start with even number\n",
    "                index = [(i-j, j) for j in range(i+1)]\n",
    "                dct_index.extend(index)\n",
    "            else:\n",
    "                index = [(j, i-j) for j in range(i+1)]\n",
    "                dct_index.extend(index)\n",
    "        for i in range(r, 2*r-1):\n",
    "            if i % 2 == 0:\n",
    "                index = [(i-j, j) for j in range(i-r+1,r)]\n",
    "                dct_index.extend(index)\n",
    "            else:\n",
    "                index = [(j, i-j) for j in range(i-r+1,r)]\n",
    "                dct_index.extend(index)\n",
    "        dct_idxs = np.asarray(dct_index)\n",
    "        return dct_idxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "67ea453e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_encoder = DctMaskEncoding(vec_dim=10000, mask_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "58d77c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average dice = 0.956: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 404/404 [01:18<00:00,  5.15it/s]\n"
     ]
    }
   ],
   "source": [
    "import losses\n",
    "valid_metric = losses.dice_metric\n",
    "\n",
    "train_loader, valid_loader, test_loader = dataset.data_loader(batch_size=1, train_transform=val_transform, val_transform=val_transform)\n",
    "average_dice = 0 \n",
    "tqdm_loader = tqdm(train_loader)\n",
    "for idx, (img_mlo, img_cc, mask1, mask2) in enumerate(tqdm_loader):\n",
    "    res = mask_encoder.encode(mask1[0])\n",
    "    restored = mask_encoder.decode(res)\n",
    "    dice = valid_metric(binarize_mask(restored[0], threshold=0.6), mask1[0][0])\n",
    "    average_dice = (average_dice * idx + dice)/(idx + 1)\n",
    "    tqdm_loader.set_description(f'average dice = {average_dice.item():.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccf4278",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_mlo: \n",
    "4096 - averge dice 0.935 - vec_dim=4096 -val_loader\n",
    "4096 - averge dice 0.941 - vec_dim=4096 - test_loader\n",
    "4096 - averge dice 0.9393 - vec_dim=4096 - train_loader\n",
    "\n",
    "mask_cc: \n",
    "4096 - average dice =0.936 - vec_dim 4096 - test_loader\n",
    "\n",
    "mask_cc: \n",
    "5000 - average dice =0.954 - vec_dim 5000 - test_loader\n",
    "5000 - average dice =0.943 - vec_dim 5000 - train_loader\n",
    "5000 - average dice =0.932 - vec_dim 5000 - val_loader\n",
    "\n",
    "\n",
    "mask_cc: \n",
    "10000 - average dice =0.958 - vec_dim 10000 - test_loader\n",
    "10000 - average dice =0.946 - vec_dim 10000 - valid_loader\n",
    "10000 - average dice =0.957 - vec_dim 10000 - train_loader\n",
    "\n",
    "\n",
    "mask_mlo:\n",
    "10000 - average dice =0.953 - vec_dim 10000 - val_loader\n",
    "10000 - average dice =0.957 - vec_dim 10000 - test_loader\n",
    "10000 - average dice =0.956 - vec_dim 10000 - train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "66999e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "9da5c50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from monai.networks.nets import UNet,UNETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "efb96f4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x648 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16,9))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.imshow(mask_mlo1[0,0])\n",
    "plt.title('mlo1')\n",
    "plt.axis('off')\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.imshow(mask_cc1[0,0])\n",
    "plt.title('cc1')\n",
    "plt.axis('off')\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.imshow(mask_cc2[0,0])\n",
    "plt.title('cc2')\n",
    "plt.axis('off')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.imshow(mask_mlo2[0,0])\n",
    "plt.title('mlo2')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9e1e7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f85c389",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b252a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6a132d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c688f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59bfe51d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16756a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8817cd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203871df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95427ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fad30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label_mlo = self.model(img_mlo)\n",
    "pred_label_cc = self.model(img_cc)\n",
    "\n",
    "vec1 = torch.sum(self.binarize_mask(pred_label_mlo), dim=(1, 2))\n",
    "vec2 = torch.sum(self.binarize_mask(pred_label_cc), dim=(1, 2))\n",
    "vec1 = vec1 / torch.norm(vec1)\n",
    "vec2 = vec2 / torch.norm(vec2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f357ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch sum in x axis = average euclid dist = 169.820 - val_loader\n",
    "                                         = 204.332 - train_loader\n",
    "                                          = 186.87 - test_loader\n",
    "        \n",
    "             y axis = ................ = 214.483 - test_loader\n",
    "                                       = 236.243 - train_loader                                                                   \n",
    "                                       = 189.567 - val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdac77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0dcd4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e98c295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d54fd99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
